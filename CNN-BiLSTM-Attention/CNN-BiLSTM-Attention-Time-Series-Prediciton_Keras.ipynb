{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1efccc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_activations(model, inputs, print_shape_only=False, layer_name=None):\n",
    "    # Documentation is available online on Github at the address below.\n",
    "    # From: https://github.com/philipperemy/keras-visualize-activations\n",
    "    print('----- activations -----')\n",
    "    activations = []\n",
    "    inp = model.input\n",
    "    if layer_name is None:\n",
    "        outputs = [layer.output for layer in model.layers]\n",
    "    else:\n",
    "        outputs = [layer.output for layer in model.layers if layer.name == layer_name]  # all layer outputs\n",
    "    funcs = [K.function([inp] + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n",
    "    layer_outputs = [func([inputs, 1.])[0] for func in funcs]\n",
    "    for layer_activations in layer_outputs:\n",
    "        activations.append(layer_activations)\n",
    "        if print_shape_only:\n",
    "            print(layer_activations.shape)\n",
    "        else:\n",
    "            print('shape为',layer_activations.shape)\n",
    "            print(layer_activations)\n",
    "    return activations\n",
    "\n",
    "\n",
    "def get_data(n, input_dim, attention_column=1):\n",
    "    \"\"\"\n",
    "    Data generation. x is purely random except that it's first value equals the target y.\n",
    "    In practice, the network should learn that the target = x[attention_column].\n",
    "    Therefore, most of its attention should be focused on the value addressed by attention_column.\n",
    "    :param n: the number of samples to retrieve.\n",
    "    :param input_dim: the number of dimensions of each element in the series.\n",
    "    :param attention_column: the column linked to the target. Everything else is purely random.\n",
    "    :return: x: model inputs, y: model targets\n",
    "    \"\"\"\n",
    "    x = np.random.standard_normal(size=(n, input_dim))\n",
    "    y = np.random.randint(low=0, high=2, size=(n, 1))\n",
    "    x[:, attention_column] = y[:, 0]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_data_recurrent(n, time_steps, input_dim, attention_column=10):\n",
    "    \"\"\"\n",
    "    Data generation. x is purely random except that it's first value equals the target y.\n",
    "    In practice, the network    should learn that the target = x[attention_column].\n",
    "    Therefore, most of its attention should be focused on the value addressed by attention_column.\n",
    "    :param n: the number of samples to retrieve.\n",
    "    :param time_steps: the number of time steps of your series.\n",
    "    :param input_dim: the number of dimensions of each element in the series.\n",
    "    :param attention_column: the column linked to the target. Everything else is purely random.\n",
    "    :return: x: model inputs, y: model targets\n",
    "    \"\"\"\n",
    "    x = np.random.standard_normal(size=(n, time_steps, input_dim))\n",
    "    y = np.random.randint(low=0, high=2, size=(n, 1))\n",
    "    x[:, attention_column, :] = np.tile(y[:], (1, input_dim))\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_data_recurrent2(n, time_steps, input_dim, attention_dim=5):\n",
    "    \"\"\"\n",
    "    假设 input_dim = 10  time_steps = 6\n",
    "    产生一个  x 6 x 10 的数据 其中每步的第 6 维 与 y相同\n",
    "\n",
    "    \"\"\"\n",
    "    x = np.random.standard_normal(size=(n, time_steps, input_dim))\n",
    "    y = np.random.randint(low=0, high=2, size=(n, 1))\n",
    "    x[:,:,attention_dim] =  np.tile(y[:], (1, time_steps))\n",
    "\n",
    "\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c0dcb3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, LSTM, Conv1D, Dropout, Bidirectional, Multiply\n",
    "from keras.layers import concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "# from attention_utils import get_activations\n",
    "from keras.layers.core import *\n",
    "from keras.layers import LSTM\n",
    "from keras.models import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18c30252",
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_ATTENTION_VECTOR = False\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = inputs\n",
    "    #a = Permute((2, 1))(inputs)\n",
    "    #a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(input_dim, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((1, 2), name='attention_vec')(a)\n",
    "\n",
    "    output_attention_mul = concatenate([inputs, a_probs], name='attention_mul')\n",
    "    return output_attention_mul\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4555dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 注意力机制的另一种写法 适合上述报错使用 来源:https://blog.csdn.net/uhauha2929/article/details/80733255\n",
    "def attention_3d_block2(inputs, single_attention_vector=False):\n",
    "    # 如果上一层是LSTM，需要return_sequences=True\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    time_steps = K.int_shape(inputs)[1]\n",
    "    input_dim = K.int_shape(inputs)[2]\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(time_steps, activation='softmax')(a)\n",
    "    if single_attention_vector:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1))(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "\n",
    "    a_probs = Permute((2, 1))(a)\n",
    "    # 乘上了attention权重，但是并没有求和，好像影响不大\n",
    "    # 如果分类任务，进行Flatten展开就可以了\n",
    "    # element-wise\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "679279fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back):\n",
    "    '''\n",
    "    对数据进行处理\n",
    "    '''\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back),:]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back,:])\n",
    "    TrainX = np.array(dataX)\n",
    "    Train_Y = np.array(dataY)\n",
    "\n",
    "    return TrainX, Train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aebdbdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#多维归一化  返回数据和最大最小值\n",
    "def NormalizeMult(data):\n",
    "    #normalize 用于反归一化\n",
    "    data = np.array(data)\n",
    "    normalize = np.arange(2*data.shape[1],dtype='float64')\n",
    "\n",
    "    normalize = normalize.reshape(data.shape[1],2)\n",
    "    print(normalize.shape)\n",
    "    for i in range(0,data.shape[1]):\n",
    "        #第i列\n",
    "        list = data[:,i]\n",
    "        listlow,listhigh =  np.percentile(list, [0, 100])\n",
    "        # print(i)\n",
    "        normalize[i,0] = listlow\n",
    "        normalize[i,1] = listhigh\n",
    "        delta = listhigh - listlow\n",
    "        if delta != 0:\n",
    "            #第j行\n",
    "            for j in range(0,data.shape[0]):\n",
    "                data[j,i]  =  (data[j,i] - listlow)/delta\n",
    "    #np.save(\"./normalize.npy\",normalize)\n",
    "    return  data,normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "88c2c760",
   "metadata": {},
   "outputs": [],
   "source": [
    "#多维反归一化\n",
    "def FNormalizeMult(data,normalize):\n",
    "    data = np.array(data)\n",
    "    for i in  range(0,data.shape[1]):\n",
    "        listlow =  normalize[i,0]\n",
    "        listhigh = normalize[i,1]\n",
    "        delta = listhigh - listlow\n",
    "        if delta != 0:\n",
    "            #第j行\n",
    "            for j in range(0,data.shape[0]):\n",
    "                data[j,i]  =  data[j,i]*delta + listlow\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "959fb456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_model():\n",
    "    inputs = Input(shape=(TIME_STEPS, INPUT_DIMS))\n",
    "\n",
    "    x = Conv1D(filters = 64, kernel_size = 1, activation = 'relu')(inputs)  #, padding = 'same'\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    #lstm_out = Bidirectional(LSTM(lstm_units, activation='relu'), name='bilstm')(x)\n",
    "    #对于GPU可以使用CuDNNLSTM\n",
    "    lstm_out = Bidirectional(LSTM(lstm_units, return_sequences=True))(x)\n",
    "    lstm_out = Dropout(0.3)(lstm_out)\n",
    "    attention_mul = attention_3d_block(lstm_out)\n",
    "    attention_mul = Flatten()(attention_mul)\n",
    "\n",
    "    output = Dense(1, activation='sigmoid')(attention_mul)\n",
    "    model = Model(inputs=[inputs], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "62801e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['pollution', 'dew', 'temp', 'press', 'wnd_spd', 'snow', 'rain'], dtype='object')\n",
      "(43800, 7)\n",
      "(7, 2)\n",
      "(43779, 20, 7) (43779, 1)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)           [(None, 20, 7)]      0           []                               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 20, 64)       512         ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 20, 64)       0           ['conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " bidirectional_5 (Bidirectional  (None, 20, 128)     66048       ['dropout_10[0][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 20, 128)      0           ['bidirectional_5[0][0]']        \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 20, 128)      16512       ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      " attention_vec (Permute)        (None, 20, 128)      0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " attention_mul (Concatenate)    (None, 20, 256)      0           ['dropout_11[0][0]',             \n",
      "                                                                  'attention_vec[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 5120)         0           ['attention_mul[0][0]']          \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1)            5121        ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 88,193\n",
      "Trainable params: 88,193\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "616/616 [==============================] - 28s 38ms/step - loss: 0.0044 - val_loss: 8.9934e-04\n",
      "Epoch 2/10\n",
      "616/616 [==============================] - 25s 41ms/step - loss: 0.0014 - val_loss: 9.1974e-04\n",
      "Epoch 3/10\n",
      "616/616 [==============================] - 23s 38ms/step - loss: 0.0012 - val_loss: 6.5019e-04\n",
      "Epoch 4/10\n",
      "616/616 [==============================] - 23s 38ms/step - loss: 0.0011 - val_loss: 6.2609e-04\n",
      "Epoch 5/10\n",
      "616/616 [==============================] - 32s 52ms/step - loss: 0.0010 - val_loss: 5.9541e-04\n",
      "Epoch 6/10\n",
      "616/616 [==============================] - 40s 65ms/step - loss: 0.0010 - val_loss: 6.1336e-04\n",
      "Epoch 7/10\n",
      "616/616 [==============================] - 25s 40ms/step - loss: 9.8625e-04 - val_loss: 5.7047e-04\n",
      "Epoch 8/10\n",
      "616/616 [==============================] - 25s 40ms/step - loss: 9.5349e-04 - val_loss: 5.5820e-04\n",
      "Epoch 9/10\n",
      "616/616 [==============================] - 23s 38ms/step - loss: 9.3372e-04 - val_loss: 5.8729e-04\n",
      "Epoch 10/10\n",
      "616/616 [==============================] - 28s 46ms/step - loss: 9.3189e-04 - val_loss: 6.6976e-04\n"
     ]
    }
   ],
   "source": [
    "#加载数据\n",
    "\n",
    "data = pd.read_csv('C:/Users/jycha/Downloads/pollution.csv')\n",
    "\n",
    "data = data.drop(['date','wnd_dir'], axis = 1)\n",
    "\n",
    "print(data.columns)\n",
    "print(data.shape)\n",
    "\n",
    "\n",
    "INPUT_DIMS = 7\n",
    "TIME_STEPS = 20\n",
    "lstm_units = 64\n",
    "\n",
    "#归一化\n",
    "data,normalize = NormalizeMult(data)\n",
    "pollution_data = data[:,0].reshape(len(data),1)\n",
    "\n",
    "train_X, _ = create_dataset(data,TIME_STEPS)\n",
    "_ , train_Y = create_dataset(pollution_data,TIME_STEPS)\n",
    "\n",
    "print(train_X.shape,train_Y.shape)\n",
    "\n",
    "m = attention_model()\n",
    "m.summary()\n",
    "m.compile(optimizer='adam', loss='mse')\n",
    "history = m.fit([train_X], train_Y, epochs=10, batch_size=64, validation_split=0.1)\n",
    "#m.save(\"./model.h5\")\n",
    "#np.save(\"normalize.npy\",normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f151813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
